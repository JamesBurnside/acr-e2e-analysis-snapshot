{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must set these constants before running this helper.\n",
    "# TEST_NAME\n",
    "# OUT_FILE\n",
    "\n",
    "\n",
    "WINDOW_SIZE = '1D'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All imports live here.\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.plotting.backend = 'plotly'\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and parse raw data.\n",
    "with open('../../data/raw/playwright.json') as f:\n",
    "    tests = pd.read_json(f)\n",
    "print('Loaded tests with %d rows. Columns:' % (tests.shape[0],))\n",
    "print(tests.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmented columns\n",
    "tests['workflow_created_date'] = tests['workflow_created_at'].dt.normalize()\n",
    "tests['test_uid'] = tests['composite'].astype(str) + '/' + tests['pw_suite_title'].astype(str) + '/' + tests['pw_spec_title'].astype(str)\n",
    "\n",
    "print(tests.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tests = tests\n",
    "# Filter to relevant tests\n",
    "cond = ((tests['workflow_event'] == 'push') & (tests['workflow_head_branch'] == 'main'))\n",
    "tests = tests[cond]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tests\n",
    "df = df[df['test_uid'] == TEST_NAME]\n",
    "df['url'] = df.apply(lambda x: 'https://github.com/Azure/communication-ui-library/actions/runs/{}'.format(x['workflow_id'],), axis=1)\n",
    "df = df.sort_values(by='workflow_created_at', ascending=False)\n",
    "df = df[df['pw_result_status'] == 'failed']\n",
    "df = df[['workflow_created_date', 'workflow_id', 'url', 'workflow_id', 'build_flavor', 'composite', 'pw_test_project_name', 'pw_result_retry']]\n",
    "with open('./{}.csv'.format(OUT_FILE), 'w') as f:\n",
    "    df.to_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summed_counts(df):\n",
    "    df = df.groupby(['workflow_created_date', 'pw_test_project_name']).count()\n",
    "    df = df['workflow_id'].unstack()\n",
    "    df = df.rolling(WINDOW_SIZE).sum()\n",
    "    return df\n",
    "    \n",
    "def pass_perc(df):\n",
    "    df = df[df['test_uid'] == TEST_NAME]\n",
    "    df_total = summed_counts(df)\n",
    "    df_failed = summed_counts(df[df['pw_result_status'] == 'failed'])\n",
    "\n",
    "    df_total = df_total.stack()\n",
    "    df_failed = df_failed.stack()\n",
    "    df_pass_perc = df_total.sub(df_failed, fill_value=0) / df_total\n",
    "    df_pass_perc.fillna(1)\n",
    "    df_pass_perc = df_pass_perc[df_pass_perc.notna()]*100\n",
    "    df_pass_perc = df_pass_perc.unstack()\n",
    "    return df_pass_perc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pass_perc = pass_perc(tests[tests['pw_result_retry'] == 2])\n",
    "\n",
    "df = df_pass_perc\n",
    "fig = df.plot(title='Success rate of test despite retries (postsubmit)', labels=dict(workflow_created_date='workflow created within last 1 days of', value='Success ratio (percent)', pw_test_project_name='Browser'))\n",
    "fig.update_traces(mode='markers+lines')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pass_perc = pass_perc(tests)\n",
    "\n",
    "df = df_pass_perc\n",
    "fig = df.plot(title='Success rate of test (postsubmit)', labels=dict(workflow_created_date='workflow created within last 1 days of', value='Success ratio (percent)', pw_test_project_name='Browser'))\n",
    "fig.update_traces(mode='markers+lines')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pass_perc = pass_perc(all_tests)\n",
    "\n",
    "# Failure of presumit tests can be for non-flakiness reasons (i.e., bad PR rejected by CI Yay!)\n",
    "# This is added just for comparison.\n",
    "df = df_pass_perc\n",
    "fig = df.plot(title='Success rate of test (including presubmit)', labels=dict(workflow_created_date='workflow created within last 1 days of', value='Success ratio (percent)', pw_test_project_name='Browser'))\n",
    "fig.update_traces(mode='markers+lines')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in tests['test_uid'].unique():\n",
    "    print(t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('e2eanalysis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c3b05de2005fd981399e64c7fa718f103e153aa6eb6a36e3fed7a3ca3494229e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
